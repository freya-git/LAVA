{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21_HES-SO-ARC_646-2.3 SCIENCE DES DONN√âES üòÅ\n",
    "## House Prices: Advanced Regression Techniques\n",
    "##### Adrien Sigrist, Vincent Zurbrugg, Loic Mary, Antoine Frey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each phase of the process:**\n",
    "1. [Business understanding](#Businessunderstanding)\n",
    "    1. [Determine the Business Objectives](#BusinessObjectives)\n",
    "    2. [Assess the Current Situation](#Assessthecurrentsituation)\n",
    "        1. [Inventory of resources](#Inventory)\n",
    "        2. [Requirements, assumptions and constraints](#Requirements)\n",
    "        3. [Risks and contingencies](#Risks)\n",
    "        4. [Terminology](#Terminology)\n",
    "        5. [Costs and benefits](#CostBenefit)\n",
    "    3. [What are the Desired Outputs](#Desiredoutputs)\n",
    "    4. [What Questions Are We Trying to Answer?](#QA)\n",
    "2. [Data Understanding](#Dataunderstanding)\n",
    "    1. [Initial Data Report](#Datareport)\n",
    "    2. [Describe Data](#Describedata)\n",
    "    3. [Initial Data Exploration](#Exploredata) \n",
    "    4. [Verify Data Quality](#Verifydataquality)\n",
    "        1. [Missing Data](#MissingData) \n",
    "        2. [Outliers](#Outliers) \n",
    "    5. [Data Quality Report](#Dataqualityreport)\n",
    "3. [Data Preparation](#Datapreparation)\n",
    "    1. [Select Your Data](#Selectyourdata)\n",
    "    2. [Cleanse the Data](#Cleansethedata)\n",
    "        1. [Label Encoding](#labelEncoding)\n",
    "        2. [Drop Unnecessary Columns](#DropCols)\n",
    "        3. [Altering Datatypes](#AlteringDatatypes)\n",
    "        4. [Dealing With Zeros](#DealingZeros)\n",
    "    3. [Construct Required Data](#Constructrequireddata)\n",
    "    4. [Integrate Data](#Integratedata)\n",
    "4. [Exploratory Data Analysis](#EDA)\n",
    "5. [Modelling](#Modelling)\n",
    "    1. [Modelling Technique](#ModellingTechnique)\n",
    "    2. [Modelling Assumptions](#ModellingAssumptions)\n",
    "    3. [Build Model](#BuildModel)\n",
    "    4. [Assess Model](#AssessModel)\n",
    "6. [Evaluation](#Evaluation)\n",
    "    1. [Evaluate Results](#EvaluateResults)\n",
    "    2. [Review Process](#ReviewProcess)\n",
    "    3. [Determine Next Steps](#NextSteps)\n",
    "7. [Deployment](#Deployment)\n",
    "    1. [Plan Deployment](#PlanDeployment)\n",
    "    2. [Plan Monitoring and Maintenance](#Monitoring)\n",
    "    3. [Produce Final Report](#FinalReport)\n",
    "    4. [Review Project](#ReviewProject)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Compr√©hension m√©tier  <a class=\"anchor\" id=\"Businessunderstanding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Les objectifs m√©tiers <a class=\"anchor\" id=\"BusinessObjectives\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pr√©dire le prix de maisons dont seul Kaggle dispose les informations de prix et cela avec la plus grande mesure de succes. \n",
    "Pour cela nous disposons d'un dataset \"train\" pour nous permettre d'entrainer notre algorithme et un dataset \"test\" pour le tester a la fin. \n",
    "\n",
    "La mesure du succ√®s ne se fait pas sur les erreurs absolues mais les erreurs relatives. C'est un √©lement important car cela veut dire que la mesure de l'erreur entre le prix estim√© et le prix r√©el se fait relativement au prix de la maison. En d'autre termes, les maisons cheres et les maisons bon march√© affecteront le r√©sultat de la meme maniere. \n",
    "\n",
    "(Exemple: 2 maisons: la premieres a 100'000 dollars et l'autre a 1'000 000 dollars, mesurer l'erreur en valeur absolue voudrait dire qu'une difference de 10'000 dollars pour les deux maisons affecterait le resultat de facon equivalente, cependant ce n'est pas le cas, la difference est mesurer en fonction du prix de la maison. La maison la plus chere affectera beaucoup moins le resultat car 10'000 dollars repr√©sente une plus petite partie de son prix. \n",
    "\n",
    "En sachant cela nous utiliseront le log du prix qui nous permettra de visualiser le prix des maisons en valeur relative. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 √âvaluer la situation actuelle<a class=\"anchor\" id=\"Assessthecurrentsituation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demandez √† un acheteur de d√©crire la maison de ses r√™ves, et il ne commencera probablement pas par la hauteur du plafond du sous-sol ou la proximit√© d'une voie ferr√©e est-ouest. Mais les donn√©es de ce concours de terrain de jeu prouvent que l'influence sur les n√©gociations de prix est bien plus importante que le nombre de chambres √† coucher ou une cl√¥ture blanche.\n",
    "\n",
    "Avec 79 variables explicatives d√©crivant (presque) tous les aspects des maisons r√©sidentielles √† Ames, dans l'Iowa, cette concurrence vous met au d√©fi de pr√©dire le prix final de chaque maison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Inventaire des ressources <a class=\"anchor\" id=\"Inventory\"></a>\n",
    "\n",
    "- Personnel: 4 personnes \n",
    "- Donn√©es: 3 jeux de donn√©es et une description texte\n",
    "- Computing resources: Nos pcs\n",
    "- Software: Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Risques et √©ventualit√©s <a class=\"anchor\" id=\"Risks\"></a>\n",
    "- Ne pas finir le projet √† temps\n",
    "- D'avoir des donn√©es inutilisables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 What are the desired outputs of the project? <a class=\"anchor\" id=\"Desiredoutputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crit√®re de succ√®s\n",
    "- Avoir la marge d'erreur la plus petite possible\n",
    "- \n",
    "\n",
    "### Crit√®re de succ√®s du minage de donn√©es\n",
    "- Avoir une pr√©diction du prix qui se rapproche le plus possible de la r√©alit√©\n",
    "- \n",
    "\n",
    "### Produire un plan de projet\n",
    "- On va utiliser la m√©thodologie CRISP DM, pour analyser le jeu de donn√©es et produire un mod√®le statistique de pr√©diction.\n",
    "\n",
    "<img src=\"image/gantt.jpg\"\n",
    "     alt=\"Gantt\"\n",
    "     style=\"height: 180px;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.4 Quelles sont les questions auxquelles nous essayons de r√©pondre ? <a class=\"anchor\" id=\"QA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pr√©dire, au plus proche de la r√©alit√©, les prix des maisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Compr√©hension des donn√©es <a class=\"anchor\" id=\"Dataunderstanding\"></a>\n",
    "\n",
    "Dans cette phase, nous allons comprendre les donn√©es que nous avons dans notre dataset. Pour relever celles qui pourront √™tre utiles et celles qui ne le seront pas. Cette phase est importante car une bonne compr√©hension des datas permet une analyse plus pr√©cise et correcte. Dans ce processus on rel√®ve le taux de valeurs manquantes dans chaque attribut, mais √©galement son type et √©ventuellement la d√©crire (quantitative, qualitative.. )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initial Data Report <a class=\"anchor\" id=\"Datareport\"></a>\n",
    "Initial data collection report - \n",
    "List the data sources acquired together with their locations, the methods used to acquire them and any problems encountered. Record problems you encountered and any resolutions achieved. This will help both with future replication of this project and with the execution of similar future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7a7bc3a05ac3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Import Libraries Required\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn.linear_model as linear_model\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import graphviz\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data source: \n",
    "# ss =  pd.read_csv('data/sample_submission.csv', sep=',') \n",
    "train_raw = pd.read_csv('data/train.csv', index_col = 'Id')\n",
    "validation_raw = pd.read_csv('data/test.csv', index_col = 'Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Description des donn√©es <a class=\"anchor\" id=\"Describedata\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation_raw\n",
    "Description du dataset validation_raw qui va permettre de valid notre mod√®le r√©aliser avec le dataset train.\n",
    "\n",
    "#### Les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validation_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les types de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Description du data set train, qui va permettre d'entrainer notre mod√®le.\n",
    "\n",
    "#### Les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les types de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify Data Quality <a class=\"anchor\" id=\"Verifydataquality\"></a>\n",
    "\n",
    "Examiner la qualit√© des donn√©es en r√©pondant √† des questions telles que:\n",
    "\n",
    "- Les donn√©es sont-elles compl√®tes (couvrent-elles tous les cas requis)?\n",
    "- Est-ce correct ou contient-il des erreurs et, s'il y a des erreurs, quelle est leur fr√©quence?\n",
    "- Y a-t-il des valeurs manquantes dans les donn√©es? Si tel est le cas, comment sont-ils repr√©sent√©s, o√π se produisent-ils et quelle est leur fr√©quence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La variable d√©pendante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette variable doit √™tre analys√©e en tout premier lieu. En effet, c'est votre raison d'√™tre dans la comp√©tition. Bonjour SalePrice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_raw['SalePrice']\n",
    "train_raw = train_raw.drop(['SalePrice'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous mettons le 'SalePrice' dans la variable y et nous le supprimons de train_raw.\n",
    "Comme c'est une variable d√©pendente nous la s√©parons pour pouvoir la r√©utiliser plus facilement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description de la variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot with normal fit\n",
    "plt.title('Normal')\n",
    "sns.distplot(y, fit = stats.norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ligne bleue repr√©sente la distribution du prix et la ligne noire est la distribution normale. \n",
    "Ce qui nous amene a la conclusion que la variable du prix n'est pas normalement distribu√©e. \n",
    "Mais c'est normal! On n'utilise pas le log du prix comme expliqu√© plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot with log normal fit\n",
    "plt.title('Log Normal')\n",
    "sns.distplot(y, fit=stats.lognorm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant le log normal, la distribution du prix est cette fois quasi normale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot with normal fit\n",
    "y = np.log1p(y)\n",
    "plt.title('Normal')\n",
    "sns.distplot(y, fit = stats.norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La m√™me ch√¥se peut √™tre dites en utilisant log1p, qui est la version du log qui selon la documentation est √† utiliser. \n",
    "https://numpy.org/doc/stable/reference/generated/numpy.log1p.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable ind√©pendantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On traite maintenant les variables ind√©pendantes du dataset. Ci-dessous, on regroupe les variables par type de variables. On note que certaines variables peuvent avoir plusieurs interpr√©tations. Par exemple, la variable _OverallQual_ est une variable √† priori qualitative ordinale. Cependant, puisqu'elle poss√®de plus que 5 niveaux, on peut la traiter comme une variable quantitative. D'autres choix sont possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
    "              'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "              'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
    "              'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "              'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n",
    "              'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
    "              'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "              'GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "              'SaleType', 'SaleCondition']\n",
    "\n",
    "quantitative = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea', 'BsmtFinSF1',  \n",
    "               'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n",
    "               'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n",
    "               'PoolArea', 'MiscVal']\n",
    "\n",
    "time = ['YearBuilt', 'YearRemodAdd', 'MoSold', 'YrSold', 'GarageYrBlt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On concat√®ne les deux dataset afin d'executer le traitement sur les donn√©es d'une traite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.concat([train_raw, validation_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Traitement des donn√©es manquantes <a class=\"anchor\" id=\"MissingData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable quantitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing(data):\n",
    "\n",
    "    missing = data.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    \n",
    "    if missing.empty:\n",
    "        print('Aucune donn√©e manquante')\n",
    "    else :\n",
    "        missing.sort_values(inplace=True)\n",
    "        missing.plot.bar()\n",
    "    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(data_raw[quantitative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_zero(data, columns):\n",
    "    \n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    for c in columns :\n",
    "        \n",
    "        if data_clean[c].isnull().any():\n",
    "            data_clean[c] = data_clean[c].fillna(0)\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable qualitative\n",
    "\n",
    "Si on lit attentivement, les donn√©es manquantes ne sont en r√©alit√© pas manquantes. Elles sont selon la documentation des __NA__ ce qui veut dire que √ßa ne fait pas de sens de parler de ladite variables dans ce cas l√†..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(data_raw[qualitative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(data, columns):\n",
    "    return pd.get_dummies(data, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien qu'il y ai des valeurs manquantes, elles ont une valeure m√©tier. \n",
    "Par exemple avoir un null (NA) dans PoolQC (la qualit√© de la piscine)\n",
    "signifie que la maison n'a pas de piscine.\n",
    "\n",
    "Il est impossible de savoir quelle proportion des valeurs null a v√©ritablement cette valeur m√©tier ou les valeurs ne sont pas renseign√©es. \n",
    "Pour les colonnes en lien avec le garage (GarageQual, GarageYrBlt, GarageType, GarageFinish, GarageCond) elles ont toutes le m√™me nombre de valeurs null ce qui indique que c'est une vraie valeur m√©tier et pas simplement un oubli. La m√™me chose peut √™tre dites pour les variables li√© au basement (sous-sol). \n",
    "\n",
    "Suite √† cette observation, nous ne supprimerons aucune colonne ou valeur null. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(data_raw[time])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'√™tre correct statistiquement parlant, on va substituer les null dans cette colonne avec la date de construction de la maison. Ces dates concordent tr√®s souvent. Il suffit de regarder le dataset. Ceci fait aussi sens d'un point de vue m√©tier. Le garage est rarement construit √† post√©riori. Il vient avec la maison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_with_column(data, missing, column) :\n",
    "    \n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    data_clean[missing] = np.where(data_clean[missing].isnull(), data_clean[column], data_clean[missing])\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, on va traiter les variables temporelles en calculant l'age relatif √† l'ann√©e de vente. D'un point de vu m√©tier, c'est probablement plus pertinent, car les ann√©es de ventes se situe entre 2006 et 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_differences_to_year_sold(data) :\n",
    "    \n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    data_clean['YearBuilt'] = data_clean['YrSold'] - data_clean['YearBuilt']\n",
    "    data_clean['YearRemodAdd'] = data_clean['YrSold'] - data_clean['YearRemodAdd']\n",
    "    data_clean['GarageYrBlt'] = data_clean['YrSold'] - data_clean['GarageYrBlt']\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisations des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_all_columns(data) :\n",
    "    \n",
    "    data_clean = data.to_numpy(copy = True)\n",
    "    \n",
    "    data_clean = StandardScaler().fit_transform(data_clean)\n",
    "    data_clean = pd.DataFrame(data_clean, index = data.index, columns = data.columns)\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, qualitative, quantitative):\n",
    "    \n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    \n",
    "    data_clean = get_dummies(data_clean, qualitative)\n",
    "    data_clean = fill_missing_with_zero(data_clean, quantitative)\n",
    "    data_clean = fill_missing_with_column(data_clean, missing = ['GarageYrBlt'], column = ['YearBuilt'])\n",
    "    \n",
    "    data_clean = compute_differences_to_year_sold(data_clean)\n",
    "    \n",
    "    data_clean = normalize_all_columns(data_clean)\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean = preprocess(data_raw, qualitative, quantitative)\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Initial Data Exploration  <a class=\"anchor\" id=\"Exploredata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va analyser le liens entre chacune des variables avec _SalePrice_. Pour ce faire, on a besoin de res√©parer le dataset qui a √©t√© nettoy√© en $X$ et $X_{val}$. La corr√©lation ne peut √™tre calcul√©e qu'entre $y$ et $X_{val}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_clean[data_clean.index.isin(train_raw.index)]\n",
    "X_val = data_clean[data_clean.index.isin(validation_raw.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va calculer les corr√©lations entre chacune des variables d√©pendantes avec la variable ind√©pendante. Dans le cas des variables quantititatives, ceci sugg√®re l'existence d'un lien lin√©aire. Dans le cas des variables dichotomiques la corr√©lation se fait interpr√©ter comme une _corr√©lation bis√©rial de point_ (https://en.wikipedia.org/wiki/Point-biserial_correlation_coefficient). \n",
    "\n",
    "En bref, cette corr√©lation effectue un t-test √† deux √©chantillons. La corr√©lation traduit la taille d'effet du t-test, qui correspond aussi √† la part de variance expliqu√©e par la variable dichotomique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(y, X, method = 'pearson'):\n",
    "    \n",
    "    cor = pd.DataFrame()\n",
    "    features = X.columns.tolist()\n",
    "   \n",
    "    cor['feature'] = X.columns.tolist()\n",
    "    cor['correlation_coef'] = [X[f].corr(y, method = 'pearson') for f in features]\n",
    "    cor['correlation_coef'] = cor['correlation_coef'].fillna(0)\n",
    "    \n",
    "    cor = cor.sort_values('correlation_coef', ascending = False)\n",
    "\n",
    "    plt.figure(figsize=(10, 0.25*len(features)))\n",
    "    sns.barplot(data = cor, y = 'feature', x = 'correlation_coef', orient = 'h')\n",
    "    \n",
    "    return cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cor = correlation(y, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Distributions  <a class=\"anchor\" id=\"Distributions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_values_table(df):\n",
    "        count_val = df.value_counts()\n",
    "        count_val_percent = 100 * df.value_counts() / len(df)\n",
    "        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n",
    "        count_val_table_ren_columns = count_val_table.rename(\n",
    "        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n",
    "        return count_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "def hist_chart(df, col):\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.hist(df[col].dropna(), edgecolor = 'k');\n",
    "        plt.xlabel(col); plt.ylabel('Number of Entries'); \n",
    "        plt.title('Distribution of '+col);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Correlations  <a class=\"anchor\" id=\"Correlations\"></a>\n",
    "En g√©n√©ral, pour r√©duire le financement, seules les variables non corr√©l√©es entre elles doivent √™tre ajout√©es aux mod√®les de r√©gression (qui sont corr√©l√©s avec le prix de vente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cor['feature'][abs(cor['correlation_coef']) > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plots(y, X, columns) :\n",
    "    \n",
    "    for f in columns :\n",
    "        x = X[f]\n",
    "        \n",
    "        plt.title('Correlation ' + y.name + ' & ' + x.name)\n",
    "        sns.regplot(x = x.name, y = y.name, data = pd.concat([x, y], axis = 1), x_jitter = .05)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter_plots(y, X, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Mod√®le baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le mod√®le baseline est le mod√®le le plus simple possible que nous avons r√©eussi √† faire et notre but sera de \n",
    "l''am√©liorer et de trouver d''autres mod√®les qui sont peu-√™tre plus performants. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ['OverallQual', 'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous prenons ces variables car ce sont celles qui sont les plus logiques quand nous choisissons un bien. \n",
    "En effet, m√™me en Suisse quand on √©stime le prix d'un bien nous regardons la qualit√© global(OverallQual), les metres carr√©es total et des diff√©rentes pieces (GrLivArea,TotalBsmtSF,1stFlrSF) et l'ann√©e de construction (YearBuilt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Regression lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(X[base]))\n",
    "res = model.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de pouvoir interpr√©ter ces r√©sultats nous devons v√©rifier les assomptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('figure', titlesize=18)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.rc('axes', titlesize=18)\n",
    "\n",
    "def graph(formula, x_range, label=None):\n",
    "    \"\"\"\n",
    "    Helper function for plotting cook's distance lines\n",
    "    \"\"\"\n",
    "    x = x_range\n",
    "    y = formula(x)\n",
    "    plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n",
    "\n",
    "\n",
    "def diagnostic_plots(X, y, model_fit=None):\n",
    "  \"\"\"\n",
    "  Function to reproduce the 4 base plots of an OLS model in R.\n",
    "\n",
    "  ---\n",
    "  Inputs:\n",
    "\n",
    "  X: A numpy array or pandas dataframe of the features to use in building the linear regression model\n",
    "\n",
    "  y: A numpy array or pandas series/dataframe of the target variable of the linear regression model\n",
    "\n",
    "  model_fit [optional]: a statsmodel.api.OLS model after regressing y on X. If not provided, will be\n",
    "                        generated from X, y\n",
    "  \"\"\"\n",
    "\n",
    "  if not model_fit:\n",
    "      model_fit = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "\n",
    "  # create dataframe from X, y for easier plot handling\n",
    "  dataframe = pd.concat([X, y], axis=1)\n",
    "\n",
    "  # model values\n",
    "  model_fitted_y = model_fit.fittedvalues\n",
    "  # model residuals\n",
    "  model_residuals = model_fit.resid\n",
    "  # normalized residuals\n",
    "  model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n",
    "  # absolute squared normalized residuals\n",
    "  model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "  # absolute residuals\n",
    "  model_abs_resid = np.abs(model_residuals)\n",
    "  # leverage, from statsmodels internals\n",
    "  model_leverage = model_fit.get_influence().hat_matrix_diag\n",
    "  # cook's distance, from statsmodels internals\n",
    "  model_cooks = model_fit.get_influence().cooks_distance[0]\n",
    "\n",
    "  plot_lm_1 = plt.figure()\n",
    "  plot_lm_1.axes[0] = sns.residplot(model_fitted_y, dataframe.columns[-1], data=dataframe,\n",
    "                            lowess=True,\n",
    "                            scatter_kws={'alpha': 0.5},\n",
    "                            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "  plot_lm_1.axes[0].set_title('Residuals vs Fitted')\n",
    "  plot_lm_1.axes[0].set_xlabel('Fitted values')\n",
    "  plot_lm_1.axes[0].set_ylabel('Residuals');\n",
    "\n",
    "  # annotations\n",
    "  abs_resid = model_abs_resid.sort_values(ascending=False)\n",
    "  abs_resid_top_3 = abs_resid[:3]\n",
    "  for i in abs_resid_top_3.index:\n",
    "      plot_lm_1.axes[0].annotate(i,\n",
    "                                 xy=(model_fitted_y[i],\n",
    "                                     model_residuals[i]));\n",
    "\n",
    "  QQ = ProbPlot(model_norm_residuals)\n",
    "  plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)\n",
    "  plot_lm_2.axes[0].set_title('Normal Q-Q')\n",
    "  plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')\n",
    "  plot_lm_2.axes[0].set_ylabel('Standardized Residuals');\n",
    "  # annotations\n",
    "  abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\n",
    "  abs_norm_resid_top_3 = abs_norm_resid[:3]\n",
    "  for r, i in enumerate(abs_norm_resid_top_3):\n",
    "      plot_lm_2.axes[0].annotate(i,\n",
    "                                 xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n",
    "                                     model_norm_residuals[i]));\n",
    "\n",
    "  plot_lm_3 = plt.figure()\n",
    "  plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5);\n",
    "  sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt,\n",
    "              scatter=False,\n",
    "              ci=False,\n",
    "              lowess=True,\n",
    "              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n",
    "  plot_lm_3.axes[0].set_title('Scale-Location')\n",
    "  plot_lm_3.axes[0].set_xlabel('Fitted values')\n",
    "  plot_lm_3.axes[0].set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n",
    "\n",
    "  # annotations\n",
    "  abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)\n",
    "  abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n",
    "  for i in abs_norm_resid_top_3:\n",
    "      plot_lm_3.axes[0].annotate(i,\n",
    "                                 xy=(model_fitted_y[i],\n",
    "                                     model_norm_residuals_abs_sqrt[i]));\n",
    "\n",
    "\n",
    "  plot_lm_4 = plt.figure();\n",
    "  plt.scatter(model_leverage, model_norm_residuals, alpha=0.5);\n",
    "  sns.regplot(model_leverage, model_norm_residuals,\n",
    "              scatter=False,\n",
    "              ci=False,\n",
    "              lowess=True,\n",
    "              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});\n",
    "  plot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)\n",
    "  plot_lm_4.axes[0].set_ylim(-3, 5)\n",
    "  plot_lm_4.axes[0].set_title('Residuals vs Leverage')\n",
    "  plot_lm_4.axes[0].set_xlabel('Leverage')\n",
    "  plot_lm_4.axes[0].set_ylabel('Standardized Residuals');\n",
    "\n",
    "  # annotations\n",
    "  leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n",
    "  for i in leverage_top_3:\n",
    "      plot_lm_4.axes[0].annotate(i,\n",
    "                                 xy=(model_leverage[i],\n",
    "                                     model_norm_residuals[i]));\n",
    "\n",
    "  p = len(model_fit.params) # number of model parameters\n",
    "  graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x),\n",
    "        np.linspace(0.001, max(model_leverage), 50),\n",
    "        'Cook\\'s distance') # 0.5 line\n",
    "  graph(lambda x: np.sqrt((1 * p * (1 - x)) / x),\n",
    "        np.linspace(0.001, max(model_leverage), 50)) # 1 line\n",
    "  plot_lm_4.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diagnostic_plots(X[base], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons ces plots pour v√©rifier les assomptions et on peut voir qu'il y a des valeurs trop extremes et qu'elles sont donc pas interpr√©tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Outliers et valeurs extr√™mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[base].iloc[[523, 1298]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons les valeurs 523 et 1298 car elles ont des valeurs trop extremes. (voir les termes exactes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(data, y, features) :\n",
    "    \n",
    "    data_clean = data.copy()\n",
    "    data_clean[y.name] = y\n",
    "    \n",
    "    for f in features :\n",
    "        condition = (data_clean[f] < -3) | (data_clean[f] > 3)\n",
    "        ix = data_clean[condition].index\n",
    "        data_clean = data_clean.drop(index = ix)\n",
    "    \n",
    "    return data_clean[data.columns], data_clean[y.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A commenter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = drop_outliers(X, y, features = base)\n",
    "\n",
    "X[base].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.3 R√©gression lin√©aire corrig√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous refaisons la r√©gression lin√©aire pour voir l''effet de la suppresion des outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(X[base]))\n",
    "res = model.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les assomptions sont respect√©es (voir section suivante) nous pouvons interpr√©ter les r√©sultats.\n",
    "\n",
    "On peut voir que les p-valeurs sont toutes en dessous de 0.05 donc ces valeurs ont peu de chances de faire une erreur sur les hypoth√®ses null. ??\n",
    "\n",
    "Parler du r-square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diagnostic_plots(sm.add_constant(X[base]), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons voir que les diagnostics plots sont bien meilleurs apr√®s la suppresion des outliers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Mod√®le features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons refaire une r√©gression lin√©aire, mais avec des param√®tres diff√©rents.En effet nous allons prendre toutes les variables qui ont plus de 0.5 de correlation positives ou n√©gatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 R√©gression lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(X[features]))\n",
    "res = model.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diagnostic_plots(X[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = drop_outliers(X, y, features = features)\n",
    "\n",
    "X[features].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 R√©gression lin√©aire corrig√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = sm.OLS(y, sm.add_constant(X[features]))\n",
    "res = model.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que le R-squared c'est am√©liorer par rapport au mod√®le de basline, cependant beaucoup de variables ont un p-valeur supp√©rieur a 0.05 donc nous devons pas les garder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =['OverallQual', 'GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt','GarageArea','Foundation_PConc','GarageCars_3.0','ExterQual_Gd','FullBath_2','Fireplaces_0','KitchenQual_TA','YearRemodAdd','GarageYrBlt','FullBath_1','ExterQual_TA']\n",
    "\n",
    "features =['OverallQual', 'GrLivArea', 'TotalBsmtSF', 'YearBuilt','GarageArea','Fireplaces_0','YearRemodAdd','GarageYrBlt']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(X[features]))\n",
    "res = model.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(sm.add_constant(X[features]), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation <a class=\"anchor\" id=\"Datapreparation\"></a>\n",
    "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Select Your Data <a class=\"anchor\" id=\"Selectyourdata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Nous cr√©ons les holdout pour entrainer notre mod√®le</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©ation des holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out pour x\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#y=np.expm1(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[features], y, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nettoyer les donn√©es <a class=\"anchor\" id=\"Cleansethedata\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Construct Required Data   <a class=\"anchor\" id=\"Constructrequireddata\"></a>\n",
    "This task includes constructive data preparation operations such as the production of derived attributes or entire new records, or transformed values for existing attributes.\n",
    "\n",
    "**Derived attributes** - These are new attributes that are constructed from one or more existing attributes in the same record, for example you might use the variables of length and width to calculate a new variable of area.\n",
    "\n",
    "**Generated records** - Here you describe the creation of any completely new records. For example you might need to create records for customers who made no purchase during the past year. There was no reason to have such records in the raw data, but for modelling purposes it might make sense to explicitly represent the fact that particular customers made zero purchases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Integrate Data  <a class=\"anchor\" id=\"Integratedata\"></a>\n",
    "These are methods whereby information is combined from multiple databases, tables or records to create new records or values.\n",
    "\n",
    "**Merged data** - Merging tables refers to joining together two or more tables that have different information about the same objects. For example a retail chain might have one table with information about each store‚Äôs general characteristics (e.g., floor space, type of mall), another table with summarised sales data (e.g., profit, percent change in sales from previous year), and another with information about the demographics of the surrounding area. Each of these tables contains one record for each store. These tables can be merged together into a new table with one record for each store, combining fields from the source tables.\n",
    "\n",
    "**Aggregations** - Aggregations refers to operations in which new values are computed by summarising information from multiple records and/or tables. For example, converting a table of customer purchases where there is one record for each purchase into a new table where there is one record for each customer, with fields such as number of purchases, average purchase amount, percent of orders charged to credit card, percent of items under promotion etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Our Primary Data Set\n",
    "Join data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis <a class=\"anchor\" id=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset has been prepared, you'll need to analyze it and summarize it's main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling <a class=\"anchor\" id=\"Modelling\"></a>\n",
    "As the first step in modelling, you'll select the actual modelling technique that you'll be using. Although you may have already selected a tool during the business understanding phase, at this stage you'll be selecting the specific modelling technique e.g. decision-tree building with C5.0, or neural network generation with back propagation. If multiple techniques are applied, perform this task separately for each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "def encode(data):\n",
    "    data_encode = data.copy()\n",
    "    for col in data:\n",
    "        encoder = LabelEncoder()\n",
    "        data_encode[col] = encoder.fit_transform(data_encode[col].astype(str))\n",
    "        X_test[col] = encoder.transform(data_encode[col].astype(str))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_advanced(data, qualitative, quantitative):\n",
    "    \n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    ## ajouter du label encoding \n",
    "    data_clean = get_dummies(data_clean, qualitative)\n",
    "    data_clean = fill_missing_with_zero(data_clean, quantitative)\n",
    "    data_clean = fill_missing_with_column(data_clean, missing = ['GarageYrBlt'], column = ['YearBuilt'])\n",
    "    \n",
    "    data_clean = compute_differences_to_year_sold(data_clean)\n",
    "    \n",
    "    data_clean = normalize_all_columns(data_clean)\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encode = encode(train_raw[qualitative])\n",
    "\n",
    "data_encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_clean =preprocess_advanced(data_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D√©finition de la fonction pour nos mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predictive_Model(estimator):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    prediction = estimator.predict(X_test)\n",
    "    print('R_squared:', metrics.r2_score(y_test, prediction))\n",
    "    print('Square Root of MSE:',np.sqrt(metrics.mean_squared_error(y_test, prediction)))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.distplot(y_test, hist=True, kde=False)\n",
    "    sns.distplot(prediction, hist=True, kde=False)\n",
    "    plt.legend(labels=['Actual Values of Price', 'Predicted Values of Price'])\n",
    "    plt.xlim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos mod√®les de pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regressor\n",
    "lr = LinearRegression()\n",
    "\n",
    "# K_Neighbor Regressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(max_depth=15, random_state=0)\n",
    "\n",
    "# Gradient Boost Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=6000,\n",
    "                                learning_rate=0.001,\n",
    "                                max_depth=7,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=15,\n",
    "                                min_samples_split=10,\n",
    "                                loss='huber',\n",
    "                                random_state=42)  \n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=1200,\n",
    "                          max_depth=15,\n",
    "                          min_samples_split=5,\n",
    "                          min_samples_leaf=5,\n",
    "                          max_features=None,\n",
    "                          oob_score=True,\n",
    "                          random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©gression lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Predictive_Model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K_Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Predictive_Model(knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arbre d√©cisionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Predictive_Model(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictive_Model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictive_Model(gbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sommaire des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regressor = ['Linear Regression', 'KNN', 'Decision Tree', 'RandomForest', 'GradientBoosting']\n",
    "models = [lr, knn, dt, rf, gbr ]\n",
    "R_squared = []\n",
    "RMSE = []\n",
    "for m in models:\n",
    "    m.fit(X_train, y_train)\n",
    "    prediction_m = m.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test, prediction_m)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction_m))\n",
    "    R_squared.append(r2)\n",
    "    RMSE.append(rmse)\n",
    "basic_result = pd.DataFrame({'R squared':R_squared,'RMSE':RMSE}, index=regressor)\n",
    "basic_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation et Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[features].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring={'R_squared':'r2','MSE':'neg_mean_squared_error'}\n",
    "kf = KFold(n_splits=12, random_state=42, shuffle=True)\n",
    "\n",
    "# Define error metrics\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X[features], y, scoring='neg_mean_squared_error', cv=kf))\n",
    "    return (rmse)\n",
    "\n",
    "def CrossVal(estimator):\n",
    "    scores = cross_validate(estimator, X[features], y, cv=kf, scoring=scoring)\n",
    "    r2 = scores['test_R_squared'].mean()\n",
    "    mse = abs(scores['test_MSE'].mean())\n",
    "    print('R_squared:', r2)\n",
    "    print('Square Root of MSE:', np.sqrt(mse))\n",
    "    \n",
    "def GridSearch(estimator, Features, Target, param_grid):\n",
    "    for key, value in scoring.items():\n",
    "        grid = GridSearchCV(estimator, param_grid, cv=10, scoring=value)\n",
    "        grid.fit(Features,Target)\n",
    "        print(key)\n",
    "        print('The Best Parameter:', grid.best_params_)\n",
    "        if grid.best_score_ > 0:\n",
    "            print('The Score:', grid.best_score_)\n",
    "        else:\n",
    "            print('The Score:', np.sqrt(abs(grid.best_score_)))\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R√©gression lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CrossVal(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K_Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = dict(n_neighbors=np.arange(5,26))\n",
    "GridSearch(KNeighborsRegressor(), X[features], y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "def ValidationCurve(estimator, Features, Target, param_name, Name_of_HyperParameter, param_range):\n",
    "    \n",
    "    train_score, test_score = validation_curve(estimator, Features, Target, param_name, param_range,cv=10,scoring='r2')\n",
    "    Rsqaured_train = train_score.mean(axis=1)\n",
    "    Rsquared_test= test_score.mean(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(param_range, Rsqaured_train, color='r', linestyle='-', marker='o', label='Training Set')\n",
    "    plt.plot(param_range, Rsquared_test, color='b', linestyle='-', marker='x', label='Testing Set')\n",
    "    plt.legend(labels=['Training Set', 'Testing Set'])\n",
    "    plt.xlabel(Name_of_HyperParameter)\n",
    "    plt.ylabel('R_squared')\n",
    "ValidationCurve(KNeighborsRegressor(), X[features], y, 'n_neighbors', 'K-Neighbors',np.arange(5,26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arbre de d√©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid=dict(max_depth=np.arange(2,15))\n",
    "GridSearch(DecisionTreeRegressor(), X[features], y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ValidationCurve(DecisionTreeRegressor(), X[features], y, 'max_depth', 'Maximum Depth', np.arange(4,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (2 m√©thodes de crossvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossVal(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cv_rmse(rf)\n",
    "print(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting (2 m√©thodes de crossvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossVal(gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = cv_rmse(gbr)\n",
    "print(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sommaire des Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scores = cross_validate(LinearRegression(), X[features], y, cv=10, scoring='r2')\n",
    "knn_scores = cross_validate(KNeighborsRegressor(n_neighbors=16), X[features], y, cv=10, scoring='r2')\n",
    "dt_scores = cross_validate(DecisionTreeRegressor(max_depth=9, random_state=0), X[features], y, cv=10, scoring='r2')\n",
    "rf_scores = cross_validate(rf, X[features], y, cv=10, scoring='r2')\n",
    "gbr_scores = cross_validate(gbr, X[features], y, cv=10, scoring='r2')\n",
    "lr_test_score = lr_scores.get('test_score')\n",
    "knn_test_score = knn_scores.get('test_score')\n",
    "dt_test_score = dt_scores.get('test_score')\n",
    "rf_test_score = rf_scores.get('test_score')\n",
    "gbr_test_score = gbr_scores.get('test_score')\n",
    "box= pd.DataFrame({'Linear Regression':lr_test_score, 'K-Nearest Neighbors':knn_test_score, 'Decision Tree':dt_test_score, 'Random Forest':rf_test_score, 'Gradient Boosting':gbr_test_score})\n",
    "box.index = box.index + 1\n",
    "box.loc['Mean'] = box.mean()\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(1,2, figsize=(20,10))\n",
    "sns.boxplot(data=box.drop(box.tail(1).index), width=0.3, palette=\"Set2\", ax=ax[0])\n",
    "ax[0].set_ylabel('R squared')\n",
    "sns.lineplot(data=box.drop(box.tail(1).index), palette=\"Set2\", ax=ax[1])\n",
    "ax[1].set_xticks(np.arange(1,11,1))\n",
    "ax[1].set_xlabel('K-th Fold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Modelling technique <a class=\"anchor\" id=\"ModellingTechnique\"></a>\n",
    "Document the actual modelling technique that is to be used.\n",
    "\n",
    "Import Models below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LinearRegression()\n",
    "#model.fit(X_train, y_train)\n",
    "#y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RandomForest')\n",
    "rf_model_full_data = rf.fit(X[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Modelling assumptions <a class=\"anchor\" id=\"ModellingAssumptions\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build Model <a class=\"anchor\" id=\"BuildModel\"></a>\n",
    "Run the modelling tool on the prepared dataset to create one or more models.\n",
    "\n",
    "**Parameter settings** - With any modelling tool there are often a large number of parameters that can be adjusted. List the parameters and their chosen values, along with the rationale for the choice of parameter settings.\n",
    "\n",
    "**Models** - These are the actual models produced by the modelling tool, not a report on the models.\n",
    "\n",
    "**Model descriptions** - Describe the resulting models, report on the interpretation of the models and document any difficulties encountered with their meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Assess Model <a class=\"anchor\" id=\"AssessModel\"></a>\n",
    "Interpret the models according to your domain knowledge, your data mining success criteria and your desired test design. Judge the success of the application of modelling and discovery techniques technically, then contact business analysts and domain experts later in order to discuss the data mining results in the business context. This task only considers models, whereas the evaluation phase also takes into account all other results that were produced in the course of the project.\n",
    "\n",
    "At this stage you should rank the models and assess them according to the evaluation criteria. You should take the business objectives and business success criteria into account as far as you can here. In most data mining projects a single technique is applied more than once and data mining results are generated with several different techniques. \n",
    "\n",
    "**Model assessment** - Summarise the results of this task, list the qualities of your generated models (e.g.in terms of accuracy) and rank their quality in relation to each other.\n",
    "\n",
    "**Revised parameter settings** - According to the model assessment, revise parameter settings and tune them for the next modelling run. Iterate model building and assessment until you strongly believe that you have found the best model(s). Document all such revisions and assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation <a class=\"anchor\" id=\"Evaluation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to final deployment of the model built by the data analyst, it is important to more thoroughly evaluate the model and review the model‚Äôs construction to be certain it properly achieves the business objectives. Here it is critical to determine if some important business issue has not been sufficiently considered. At the end of this phase, the project leader then should decide exactly how to use the data mining results. The key steps here are the evaluation of results, the process review, and the determination of next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Evaluate Results <a class=\"anchor\" id=\"EvaluateResults\"></a>\n",
    "\n",
    "Previous evaluation steps dealt with factors such as the accuracy and generality of the model. This step assesses the degree to which the model meets the business objectives and determines if there is some business reason why this model is deficient. Another option here is to test the model(s) on real-world applications‚Äîif time and budget constraints permit. Moreover, evaluation also seeks to unveil additional challenges, information, or hints for future directions.\n",
    "\n",
    "At this stage, the data analyst summarizes the assessment results in terms of business success criteria, including a final statement about whether the project already meets the initial\n",
    "business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Utilisation du gradient boosting car il est le mod√®le le plus pr√©cis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G√©n√©ration du mod√®le avec gradientboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pr√©diction de y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = gbr_model_full_data.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Root Mean square error: \" , np.sqrt(metrics.mean_squared_error(y_test,y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G√©n√©ration des valeurs pr√©dites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gbr_model_full_data.predict(X_val[features])\n",
    "\n",
    "output = pd.DataFrame({'Id': X_val.index, 'SalePrice': np.expm1(predictions)})\n",
    "output.to_csv('Data/my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Review Process <a class=\"anchor\" id=\"ReviewProcess\"></a>\n",
    "\n",
    "It is now appropriate to do a more thorough review of the data mining engagement to determine if there is any important factor or task that has somehow been overlooked. This review also covers quality assurance issues (e.g., did we correctly build the model? Did we only use allowable attributes that are available for future deployment?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Determine Next Steps <a class=\"anchor\" id=\"NextSteps\"></a>\n",
    "\n",
    "At this stage, the project leader must decide whether to finish this project and move on to deployment or whether to initiate further iterations or set up new data mining projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Deployment  <a class=\"anchor\" id=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation is generally not the end of the project. The knowledge gained must be organized and presented in a way that the customer can use it, which often involves applying ‚Äúlive‚Äù models within an organization‚Äôs decision-making processes, such as the real-time personalization of Web pages or repeated scoring of marketing databases.\n",
    "\n",
    "Depending on the requirements, the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data mining process across the enterprise. Even though it is often the customer, not the data analyst, who carries out the deployment steps, it is important for the customer to understand up front what actions must be taken in order to actually make use of the created models. The key steps here are plan deployment, plan monitoring and maintenance, the production of the final report, and review of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Plan Deployment <a class=\"anchor\" id=\"PlanDeployment\"></a>\n",
    "\n",
    "In order to deploy the data mining result(s) into the business, this task takes the evaluation results and develops a strategy for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Plan Monitoring and Maintenance <a class=\"anchor\" id=\"Monitoring\"></a>\n",
    "Monitoring and maintenance are important issues if the data mining result is to become part of the day-to-day business and its environment. A carefully prepared maintenance strategy avoids incorrect usage of data mining results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Produce Final Report <a class=\"anchor\" id=\"FinalReport\"></a>\n",
    "At the end of the project, the project leader and his or her team write up a final report. Depending on the deployment plan, this report may be only a summary of the project and its experiences (if they have not already been documented as an ongoing activity) or it may be a final and comprehensive presentation of the data mining result(s). This report includes all of the previous deliverables and summarizes and organizes the results. Also, there often will be a meeting at the conclusion of the project, where the results are verbally presented to the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Review Project <a class=\"anchor\" id=\"ReviewProject\"></a>\n",
    "The data analyst should assess failures and successes as well as potential areas of improvement for use in future projects. This step should include a summary of important experiences during the project and can include interviews with the significant project participants. This document could include pitfalls, misleading approaches, or hints for selecting the best-suited data mining techniques in similar situations. In ideal projects, experience documentation also covers any reports written by individual project members during the project phases and tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
